---
title: "analyze"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r}
rm(list = ls())
library(data.table); library(tidyverse); library(correlation)
library(BayesFactor); library(BFpack); library(ggbeeswarm)
theme_set(theme_minimal())
```

```{r}
d0_feed <- fread("../data/clean/data_feed.csv")


d1 <- fread("../data/clean/data.csv")
d1[, n_distinct(user_id), keyby = .(condition)]
d1[, n_distinct(user_id), keyby = .(condition, attention, ipblock)]  # users per condition

d2 <- d1[attention == 1.0 & ipblock == 0]
d2[, n_distinct(user_id), keyby = .(condition, attention, ipblock)]

# remove alphanumeric response column, ip address and country codes
d2 <- select(d2, user_id, condition, field, news_eval)
perc <- d2[field == "willsmith_cards_perc", .(user_id, news_eval)]
setnames(perc, c("user_id", "willsmith_perc"))
perc

d2 <- d2[field != "willsmith_cards_perc"] |> left_join(perc) 
setDT(d2)
```

```{r}
d2 <- separate(d2, field, c("topic", "question"), "_")
d3 <- dcast(d2, user_id + condition + topic + willsmith_perc ~ question, value.var = "news_eval")
d3[, rating_combined := (importance + newsworthiness) / 2]
d3
```

```{r check correlations}
cors <- d3[, .(r = cor(importance, newsworthiness)), topic]
cors
ggplot(d3, aes(importance, newsworthiness)) +
    facet_wrap(~ topic) +
    geom_point() +
    geom_smooth(method = 'lm', alpha = 0.1) 

select(d3, topic, importance, newsworthiness) %>% group_by(topic) %>% correlation()
```


```{r}
d4 <- distinct(d3[, .(user_id, condition, willsmith_perc)])
d4[, condition := condition + 0.5]
summary(lm(willsmith_perc ~ condition, d4))

p1 <- ggplot(d4, aes(factor(condition, labels = c("control", "treatment")), willsmith_perc)) +
    geom_quasirandom(alpha = 0.3, size = 0.8) +
    stat_summary(fun = mean, geom = 'point', shape = 95, size = 6) +
    stat_summary(fun.data = mean_cl_normal, geom = 'errorbar', width = 0, size = 1.1) +
    scale_y_continuous(limits = c(0, 100)) +
    labs(x = "", y = "self-reported percent will smith cards seen")
p1

ggsave("../figures/manipulation_check.png", p1, dpi = 300, width = 5, height = 5, bg = "white")
```


```{r fit models}
# if cor is < 0.7 interpret this
ttestBF(formula = importance ~ condition, data = d3[topic == "willsmith"])
m1_imp <- lm(importance ~ condition, d3[topic == "willsmith"])
summary(m1_imp)
m1_impbf <- BF(m1_imp, hypothesis = "condition = 0")
summary(m1_impbf)

ttestBF(formula = newsworthiness ~ condition, data = d3[topic == "willsmith"])
m1_newsw <- lm(newsworthiness ~ condition, d3[topic == "willsmith"])
summary(m1_newsw)
m1_newswbf <- BF(m1_newsw, hypothesis = "condition = 0")
summary(m1_newswbf)

# if cor > 0.7, interpret this
ttestBF(formula = rating_combined ~ condition, data = d3[topic == "willsmith"])
m1_combine <- lm(rating_combined ~ condition, d3[topic == "willsmith"])
summary(m1_combine)
m1_combinebf <- BF(m1_combine, hypothesis = "condition = 0")
summary(m1_combinebf)
```


```{r}
dodge <- 0.8
p2 <- ggplot(d2, aes(question, news_eval, col = factor(condition, labels = c("control", "treatment")))) +
    facet_wrap(~ topic) +
    geom_quasirandom(alpha = 0.3, dodge = dodge, size = 0.8) +
    stat_summary(fun = mean, geom = 'point', shape = 95, size = 6, position = position_dodge(dodge)) +
    stat_summary(fun.data = mean_cl_normal, geom = 'errorbar', width = 0, size = 1.1, position = position_dodge(dodge)) +
    scale_y_continuous(limits = c(0, 7), breaks = 1:6) +
    labs(x = "", y = "rating", col = "condition")
p2

ggsave("../figures/main_effect.png", p2, dpi = 300, width = 13, height = 5, bg = "white")
```




```{r}
# ggdist
# https://mjskay.github.io/ggdist/articles/dotsinterval.html
```

































# other


```{r}
glimpse(d0_feed)
d0_feed$user_agent <- NULL
d0_feed$entity_url <- NULL

# do people who spend more time lookin gat will msmith cards differ in anyway?

d0_feed[, unique(entity_set)]
d0_feed[, unique(engagement_type)]

d0_feed <- d0_feed[entity_set == "22apr25_willsmith"]

d0_feed[engagement_type == "dwell" & response < 30000, ]

d0_feed[engagement_type == "dwell" & response < 30000 & !item_order %in% c(0, 1, 2, 97, 98, 99)]

dt_dwell <- d0_feed[engagement_type == "dwell" & response < 30000, 
        .(n_dwell = .N, 
          dwell_mean = mean(response),
          dwell_sum = sum(response)
          ), 
        keyby = .(condition, user_id)]
dt_dwell

# likes and shares
d0_feed[engagement_type == "dwell" & response < 30000, 
        .(n_dwell = .N, 
          dwell_mean = mean(response),
          dwell_sum = sum(response)
          ), 
        keyby = .(condition, user_id)]



d5 <- left_join(d4, select(dt_dwell, -condition))
setDT(d5)

summary(lm(willsmith_perc ~ n_dwell, d5))
correlation(select(d5[condition == 1], willsmith_perc, n_dwell, dwell_mean, dwell_sum), p_adjust = "none")


ggplot(d5, aes(dwell_mean, willsmith_perc)) +
    geom_point() +
    geom_smooth(method = 'lm', alpha = 0.1) 






d0_feed[engagement_type != "dwell", .(user_id, item_order, entity_id, engagement_type, response)][order(user_id, item_order)] |> View()
d0_feed[engagement_type != "dwell", unique(response)]


d0_feed[engagement_type != "dwell" & response == 1, ]

d0_feed[engagement_type == "likes", sum(response), .(user_id, entity_id)][V1 > 1]
d0_feed[engagement_type == "shares", sum(response), .(user_id, entity_id)][V1 > 1]
d0_feed[engagement_type == "read_later", sum(response), .(user_id, entity_id)][V1 > 1]

library(fixest)
feols(response ~ likes | user_id, d0_feed[engagement_type == "dwell" & response < 10000])
feols(response ~ shares | user_id, d0_feed[engagement_type == "dwell" & response < 10000])
```

